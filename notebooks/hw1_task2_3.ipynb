{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q mrjob nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2d90d2029b984b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mrjob.job import MRStep, MRJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6379df03b233d258",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/staticanalysis.root.20240207.010351.083779\n",
      "Running step 1 of 2...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/staticanalysis.root.20240207.010351.083779/output\n",
      "Streaming final output from /tmp/staticanalysis.root.20240207.010351.083779/output...\n",
      "Removing temp directory /tmp/staticanalysis.root.20240207.010351.083779...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/staticanalysis.root.20240207.010353.827057\n",
      "Running step 1 of 2...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/staticanalysis.root.20240207.010353.827057/output\n",
      "Streaming final output from /tmp/staticanalysis.root.20240207.010353.827057/output...\n",
      "Removing temp directory /tmp/staticanalysis.root.20240207.010353.827057...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/staticanalysis.root.20240207.010356.658513\n",
      "Running step 1 of 2...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/staticanalysis.root.20240207.010356.658513/output\n",
      "Streaming final output from /tmp/staticanalysis.root.20240207.010356.658513/output...\n",
      "Removing temp directory /tmp/staticanalysis.root.20240207.010356.658513...\n"
     ]
    }
   ],
   "source": [
    "!python3 staticanalysis.py SW_EpisodeIV.txt > SW_EpisodeIV-p3.csv\n",
    "!python3 staticanalysis.py SW_EpisodeV.txt > SW_EpisodeV-p3.csv\n",
    "!python3 staticanalysis.py SW_EpisodeVI.txt > SW_EpisodeVI-p3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3694a005-ee5d-4e4d-824e-33ea3fac9449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SW_EpisodeIV\n",
      "                    bigram  count\n",
      "0         [\"im\", \"going\"]     13\n",
      "1    [\"obiwan\", \"kenobi\"]     12\n",
      "2       [\"rebel\", \"base\"]     11\n",
      "3          [\"r2\", \"unit\"]     11\n",
      "4        [\"dont\", \"know\"]     11\n",
      "5      [\"sand\", \"people\"]     10\n",
      "6          [\"ive\", \"got\"]     10\n",
      "7   [\"battle\", \"station\"]     10\n",
      "8        [\"luke\", \"luke\"]      9\n",
      "9       [\"looks\", \"like\"]      9\n",
      "10  [\"intercom\", \"voice\"]      9\n",
      "11     [\"youre\", \"going\"]      8\n",
      "12      [\"uncle\", \"owen\"]      7\n",
      "13   [\"star\", \"intercom\"]      7\n",
      "14      [\"lord\", \"vader\"]      7\n",
      "15      [\"dont\", \"worry\"]      7\n",
      "16        [\"weve\", \"got\"]      6\n",
      "17      [\"red\", \"leader\"]      6\n",
      "18        [\"know\", \"hes\"]      6\n",
      "19      [\"dont\", \"think\"]      6\n",
      "\n",
      "SW_EpisodeV\n",
      "                       bigram  count\n",
      "0          [\"lord\", \"vader\"]     14\n",
      "1            [\"im\", \"going\"]     12\n",
      "2           [\"dont\", \"know\"]     12\n",
      "3            [\"yes\", \"lord\"]      9\n",
      "4           [\"dark\", \"side\"]      7\n",
      "5      [\"asteroid\", \"field\"]      7\n",
      "6           [\"rouge\", \"two\"]      6\n",
      "7         [\"master\", \"luke\"]      6\n",
      "8        [\"captain\", \"solo\"]      6\n",
      "9       [\"bounty\", \"hunter\"]      6\n",
      "10          [\"take\", \"care\"]      5\n",
      "11  [\"millennium\", \"falcon\"]      5\n",
      "12          [\"im\", \"trying\"]      5\n",
      "13            [\"im\", \"sure\"]      5\n",
      "14           [\"im\", \"sorry\"]      5\n",
      "15      [\"energy\", \"shield\"]      5\n",
      "16            [\"yes\", \"sir\"]      4\n",
      "17           [\"weve\", \"got\"]      4\n",
      "18          [\"wait\", \"wait\"]      4\n",
      "19        [\"turn\", \"around\"]      4\n",
      "\n",
      "SW_EpisodeVI\n",
      "                      bigram  count\n",
      "0          [\"dark\", \"side\"]     12\n",
      "1        [\"master\", \"luke\"]     11\n",
      "2             [\"gon\", \"na\"]     11\n",
      "3              [\"oh\", \"oh\"]      8\n",
      "4         [\"death\", \"star\"]      8\n",
      "5        [\"gold\", \"leader\"]      6\n",
      "6          [\"dont\", \"know\"]      6\n",
      "7       [\"captain\", \"solo\"]      5\n",
      "8   [\"shuttle\", \"tydirium\"]      4\n",
      "9   [\"shield\", \"generator\"]      4\n",
      "10           [\"oh\", \"dear\"]      4\n",
      "11        [\"lord\", \"vader\"]      4\n",
      "12        [\"jabba\", \"hutt\"]      4\n",
      "13      [\"general\", \"solo\"]      4\n",
      "14        [\"dont\", \"think\"]      4\n",
      "15         [\"dont\", \"move\"]      4\n",
      "16  [\"deflector\", \"shield\"]      4\n",
      "17       [\"darth\", \"vader\"]      4\n",
      "18         [\"come\", \"back\"]      4\n",
      "19       [\"artoo\", \"artoo\"]      4\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('SW_EpisodeIV-p3.csv', sep=\"\\t\", names=['bigram', 'count'], nrows=20)\n",
    "print(\"\\nSW_EpisodeIV\\n\", df1)\n",
    "df2 = pd.read_csv('SW_EpisodeV-p3.csv', sep=\"\\t\", names=['bigram', 'count'], nrows=20)\n",
    "print(\"\\nSW_EpisodeV\\n\", df2)\n",
    "df3 = pd.read_csv('SW_EpisodeVI-p3.csv', sep=\"\\t\", names=['bigram', 'count'], nrows=20)\n",
    "print(\"\\nSW_EpisodeVI\\n\", df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cdf988-dd32-4737-bac7-f2cab915564b",
   "metadata": {},
   "source": [
    "## Исполним на кластере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e259588-9a66-46e7-939e-0b660d3c3964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/staticanalysis.root.20240207.010829.698867\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/staticanalysis.root.20240207.010829.698867/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/staticanalysis.root.20240207.010829.698867/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8987775868236673199/] [] /tmp/streamjob680202584790095178.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1707060155216_0019\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1707060155216_0019\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1707060155216_0019\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1707060155216_0019/\n",
      "  Running job: job_1707060155216_0019\n",
      "  Job job_1707060155216_0019 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1707060155216_0019_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:466)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:350)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1707060155216_0019 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/staticanalysis.root.20240207.010829.698867/step-output/0000\n",
      "Counters: 57\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=82374\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=95268\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=117282\n",
      "\t\tFILE: Number of bytes written=1365874\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=82560\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=95268\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tOther local map tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=39691264\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11925504\n",
      "\t\tTotal time spent by all map tasks (ms)=38761\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=77522\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11646\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23292\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=38761\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=11646\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4120\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=567\n",
      "\t\tInput split bytes=186\n",
      "\t\tMap input records=1011\n",
      "\t\tMap output bytes=107332\n",
      "\t\tMap output materialized bytes=117294\n",
      "\t\tMap output records=4969\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPeak Map Physical memory (bytes)=288239616\n",
      "\t\tPeak Map Virtual memory (bytes)=2571190272\n",
      "\t\tPeak Reduce Physical memory (bytes)=193220608\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2577170432\n",
      "\t\tPhysical memory (bytes) snapshot=927686656\n",
      "\t\tReduce input groups=4377\n",
      "\t\tReduce input records=4969\n",
      "\t\tReduce output records=4377\n",
      "\t\tReduce shuffle bytes=117294\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=9938\n",
      "\t\tTotal committed heap usage (bytes)=761790464\n",
      "\t\tVirtual memory (bytes) snapshot=10295046144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7069984964533525003/] [] /tmp/streamjob3924684846459586370.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1707060155216_0020\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1707060155216_0020\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1707060155216_0020\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1707060155216_0020/\n",
      "  Running job: job_1707060155216_0020\n",
      "  Job job_1707060155216_0020 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1707060155216_0020 completed successfully\n",
      "  Output directory: hdfs:///hw1/episode4-p3\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=95268\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=410\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=139050\n",
      "\t\tFILE: Number of bytes written=1409326\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=95584\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=410\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12670976\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11395072\n",
      "\t\tTotal time spent by all map tasks (ms)=12374\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24748\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11128\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=22256\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12374\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=11128\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2790\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=173\n",
      "\t\tInput split bytes=316\n",
      "\t\tMap input records=4377\n",
      "\t\tMap output bytes=130284\n",
      "\t\tMap output materialized bytes=139062\n",
      "\t\tMap output records=4377\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPeak Map Physical memory (bytes)=296288256\n",
      "\t\tPeak Map Virtual memory (bytes)=2570944512\n",
      "\t\tPeak Reduce Physical memory (bytes)=188854272\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2576596992\n",
      "\t\tPhysical memory (bytes) snapshot=963481600\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=4377\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=139062\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=8754\n",
      "\t\tTotal committed heap usage (bytes)=794820608\n",
      "\t\tVirtual memory (bytes) snapshot=10292162560\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///hw1/episode4-p3\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/staticanalysis.root.20240207.010829.698867...\n",
      "Removing temp directory /tmp/staticanalysis.root.20240207.010829.698867...\n"
     ]
    }
   ],
   "source": [
    "!python3 staticanalysis.py -r hadoop hdfs://namenode:8020/hw1/SW_EpisodeIV.txt --conf-path cfg3.conf --output /hw1/episode4-p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f495baa-02a5-4a0e-b8d0-6dc91c313a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/staticanalysis.root.20240207.011211.374088\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/staticanalysis.root.20240207.011211.374088/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/staticanalysis.root.20240207.011211.374088/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6216724864038694268/] [] /tmp/streamjob3983836633726272716.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1707060155216_0021\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1707060155216_0021\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1707060155216_0021\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1707060155216_0021/\n",
      "  Running job: job_1707060155216_0021\n",
      "  Job job_1707060155216_0021 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1707060155216_0021 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/staticanalysis.root.20240207.011211.374088/step-output/0000\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=59583\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=64475\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=78261\n",
      "\t\tFILE: Number of bytes written=1287832\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=59767\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=64475\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13126656\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12066816\n",
      "\t\tTotal time spent by all map tasks (ms)=12819\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=25638\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11784\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23568\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12819\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=11784\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3180\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=166\n",
      "\t\tInput split bytes=184\n",
      "\t\tMap input records=840\n",
      "\t\tMap output bytes=71627\n",
      "\t\tMap output materialized bytes=78273\n",
      "\t\tMap output records=3311\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPeak Map Physical memory (bytes)=313495552\n",
      "\t\tPeak Map Virtual memory (bytes)=2570059776\n",
      "\t\tPeak Reduce Physical memory (bytes)=258936832\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2582159360\n",
      "\t\tPhysical memory (bytes) snapshot=1040826368\n",
      "\t\tReduce input groups=2961\n",
      "\t\tReduce input records=3311\n",
      "\t\tReduce output records=2961\n",
      "\t\tReduce shuffle bytes=78273\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=6622\n",
      "\t\tTotal committed heap usage (bytes)=833617920\n",
      "\t\tVirtual memory (bytes) snapshot=10296971264\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6007538379565206186/] [] /tmp/streamjob7574436861732695466.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1707060155216_0022\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1707060155216_0022\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1707060155216_0022\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1707060155216_0022/\n",
      "  Running job: job_1707060155216_0022\n",
      "  Job job_1707060155216_0022 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1707060155216_0022 completed successfully\n",
      "  Output directory: hdfs:///hw1/episode5-p3\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=64475\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=404\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=94097\n",
      "\t\tFILE: Number of bytes written=1319420\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=64791\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=404\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12051456\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10957824\n",
      "\t\tTotal time spent by all map tasks (ms)=11769\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=23538\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10701\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=21402\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11769\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10701\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3000\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=179\n",
      "\t\tInput split bytes=316\n",
      "\t\tMap input records=2961\n",
      "\t\tMap output bytes=88163\n",
      "\t\tMap output materialized bytes=94109\n",
      "\t\tMap output records=2961\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPeak Map Physical memory (bytes)=292433920\n",
      "\t\tPeak Map Virtual memory (bytes)=2572300288\n",
      "\t\tPeak Reduce Physical memory (bytes)=190156800\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2578071552\n",
      "\t\tPhysical memory (bytes) snapshot=949297152\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=2961\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=94109\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=5922\n",
      "\t\tTotal committed heap usage (bytes)=765460480\n",
      "\t\tVirtual memory (bytes) snapshot=10295279616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///hw1/episode5-p3\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/staticanalysis.root.20240207.011211.374088...\n",
      "Removing temp directory /tmp/staticanalysis.root.20240207.011211.374088...\n"
     ]
    }
   ],
   "source": [
    "!python3 staticanalysis.py -r hadoop hdfs://namenode:8020/hw1/SW_EpisodeV.txt --conf-path cfg3.conf --output /hw1/episode5-p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66d54f15-4d38-44a3-8dab-6e2e228a7a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/staticanalysis.root.20240207.011427.179549\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/staticanalysis.root.20240207.011427.179549/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/staticanalysis.root.20240207.011427.179549/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar5933415541429222977/] [] /tmp/streamjob8989343103376032212.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1707060155216_0023\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1707060155216_0023\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1707060155216_0023\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1707060155216_0023/\n",
      "  Running job: job_1707060155216_0023\n",
      "  Job job_1707060155216_0023 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1707060155216_0023 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/staticanalysis.root.20240207.011427.179549/step-output/0000\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=52272\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=57034\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=68743\n",
      "\t\tFILE: Number of bytes written=1268800\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=52458\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=57034\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13700096\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=13166592\n",
      "\t\tTotal time spent by all map tasks (ms)=13379\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=26758\n",
      "\t\tTotal time spent by all reduce tasks (ms)=12858\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=25716\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13379\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=12858\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3580\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=164\n",
      "\t\tInput split bytes=186\n",
      "\t\tMap input records=675\n",
      "\t\tMap output bytes=62909\n",
      "\t\tMap output materialized bytes=68755\n",
      "\t\tMap output records=2911\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPeak Map Physical memory (bytes)=296493056\n",
      "\t\tPeak Map Virtual memory (bytes)=2574237696\n",
      "\t\tPeak Reduce Physical memory (bytes)=232333312\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2581897216\n",
      "\t\tPhysical memory (bytes) snapshot=972288000\n",
      "\t\tReduce input groups=2622\n",
      "\t\tReduce input records=2911\n",
      "\t\tReduce output records=2622\n",
      "\t\tReduce shuffle bytes=68755\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=5822\n",
      "\t\tTotal committed heap usage (bytes)=754974720\n",
      "\t\tVirtual memory (bytes) snapshot=10304503808\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar1152552771509806746/] [] /tmp/streamjob6491453498443350790.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1707060155216_0024\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1707060155216_0024\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1707060155216_0024\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1707060155216_0024/\n",
      "  Running job: job_1707060155216_0024\n",
      "  Job job_1707060155216_0024 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1707060155216_0024 completed successfully\n",
      "  Output directory: hdfs:///hw1/episode6-p3\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57034\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=413\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83266\n",
      "\t\tFILE: Number of bytes written=1297758\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=57350\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=413\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10923008\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10877952\n",
      "\t\tTotal time spent by all map tasks (ms)=10667\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=21334\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10623\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=21246\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10667\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10623\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2540\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=165\n",
      "\t\tInput split bytes=316\n",
      "\t\tMap input records=2622\n",
      "\t\tMap output bytes=78010\n",
      "\t\tMap output materialized bytes=83278\n",
      "\t\tMap output records=2622\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPeak Map Physical memory (bytes)=297058304\n",
      "\t\tPeak Map Virtual memory (bytes)=2571223040\n",
      "\t\tPeak Reduce Physical memory (bytes)=191545344\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2577821696\n",
      "\t\tPhysical memory (bytes) snapshot=965754880\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=2622\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=83278\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=5244\n",
      "\t\tTotal committed heap usage (bytes)=771227648\n",
      "\t\tVirtual memory (bytes) snapshot=10296188928\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///hw1/episode6-p3\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/staticanalysis.root.20240207.011427.179549...\n",
      "Removing temp directory /tmp/staticanalysis.root.20240207.011427.179549...\n"
     ]
    }
   ],
   "source": [
    "!python3 staticanalysis.py -r hadoop hdfs://namenode:8020/hw1/SW_EpisodeVI.txt --conf-path cfg3.conf --output /hw1/episode6-p3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a1183-35f5-440c-be63-56b3f1cde7aa",
   "metadata": {},
   "source": [
    "### Проверка корректности результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "437a99f3-cc56-42c6-a8e0-21b1ef267c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================ IV ===============\n",
      "\n",
      "\n",
      "[\"im\", \"going\"]\t13\n",
      "[\"obiwan\", \"kenobi\"]\t12\n",
      "[\"rebel\", \"base\"]\t11\n",
      "[\"r2\", \"unit\"]\t11\n",
      "[\"dont\", \"know\"]\t11\n",
      "[\"sand\", \"people\"]\t10\n",
      "[\"ive\", \"got\"]\t10\n",
      "[\"battle\", \"station\"]\t10\n",
      "[\"luke\", \"luke\"]\t9\n",
      "[\"looks\", \"like\"]\t9\n",
      "[\"intercom\", \"voice\"]\t9\n",
      "[\"youre\", \"going\"]\t8\n",
      "[\"uncle\", \"owen\"]\t7\n",
      "[\"star\", \"intercom\"]\t7\n",
      "[\"lord\", \"vader\"]\t7\n",
      "[\"dont\", \"worry\"]\t7\n",
      "[\"weve\", \"got\"]\t6\n",
      "[\"red\", \"leader\"]\t6\n",
      "[\"know\", \"hes\"]\t6\n",
      "[\"dont\", \"think\"]\t6\n",
      "\n",
      "\n",
      "================ V ================\n",
      "\n",
      "\n",
      "[\"lord\", \"vader\"]\t14\n",
      "[\"im\", \"going\"]\t12\n",
      "[\"dont\", \"know\"]\t12\n",
      "[\"yes\", \"lord\"]\t9\n",
      "[\"dark\", \"side\"]\t7\n",
      "[\"asteroid\", \"field\"]\t7\n",
      "[\"rouge\", \"two\"]\t6\n",
      "[\"master\", \"luke\"]\t6\n",
      "[\"captain\", \"solo\"]\t6\n",
      "[\"bounty\", \"hunter\"]\t6\n",
      "[\"take\", \"care\"]\t5\n",
      "[\"millennium\", \"falcon\"]\t5\n",
      "[\"im\", \"trying\"]\t5\n",
      "[\"im\", \"sure\"]\t5\n",
      "[\"im\", \"sorry\"]\t5\n",
      "[\"energy\", \"shield\"]\t5\n",
      "[\"yes\", \"sir\"]\t4\n",
      "[\"weve\", \"got\"]\t4\n",
      "[\"wait\", \"wait\"]\t4\n",
      "[\"turn\", \"around\"]\t4\n",
      "\n",
      "\n",
      "================ VI ===============\n",
      "\n",
      "\n",
      "[\"dark\", \"side\"]\t12\n",
      "[\"master\", \"luke\"]\t11\n",
      "[\"gon\", \"na\"]\t11\n",
      "[\"oh\", \"oh\"]\t8\n",
      "[\"death\", \"star\"]\t8\n",
      "[\"gold\", \"leader\"]\t6\n",
      "[\"dont\", \"know\"]\t6\n",
      "[\"captain\", \"solo\"]\t5\n",
      "[\"shuttle\", \"tydirium\"]\t4\n",
      "[\"shield\", \"generator\"]\t4\n",
      "[\"oh\", \"dear\"]\t4\n",
      "[\"lord\", \"vader\"]\t4\n",
      "[\"jabba\", \"hutt\"]\t4\n",
      "[\"general\", \"solo\"]\t4\n",
      "[\"dont\", \"think\"]\t4\n",
      "[\"dont\", \"move\"]\t4\n",
      "[\"deflector\", \"shield\"]\t4\n",
      "[\"darth\", \"vader\"]\t4\n",
      "[\"come\", \"back\"]\t4\n",
      "[\"artoo\", \"artoo\"]\t4\n"
     ]
    }
   ],
   "source": [
    "!echo \"\\n\\n================ IV ===============\\n\\n\"\n",
    "!hadoop fs -cat /hw1/episode4-p3/part-00000\n",
    "!echo \"\\n\\n================ V ================\\n\\n\"\n",
    "!hadoop fs -cat /hw1/episode5-p3/part-00000\n",
    "!echo \"\\n\\n================ VI ===============\\n\\n\"\n",
    "!hadoop fs -cat /hw1/episode6-p3/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c7031b-7fe8-49bf-bb4d-4c60845433dc",
   "metadata": {},
   "source": [
    "### Листинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "846f5710-58a3-4f32-85ab-6b3f910f7825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Config: ===========\n",
      "runners:\n",
      "  hadoop:\n",
      "    setup:\n",
      "        'pip install nltk'\n",
      "    jobconf:\n",
      "      mapreduce.job.reduces: 2\n",
      "========== Count:  ===========\n",
      "import string\n",
      "\n",
      "from mrjob.job import MRStep, MRJob\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "\n",
      "class StaticAnalysis(MRJob):\n",
      "    def map_init(self):\n",
      "        import nltk\n",
      "        nltk.download('punkt')\n",
      "        nltk.download('stopwords')\n",
      "\n",
      "    def map(self, _, line):\n",
      "        parsed = (line.lower()).translate(str.maketrans('', '', string.punctuation)).split()[1:]\n",
      "\n",
      "        phrase = ' '.join(parsed[1:])\n",
      "        cleaned = [word for word in word_tokenize(phrase) if word not in set(stopwords.words('english'))]\n",
      "        bigrams = [(cleaned[i], cleaned[i + 1]) for i in range(len(cleaned) - 1)]\n",
      "        for bigram in bigrams:\n",
      "            yield bigram, 1\n",
      "\n",
      "    def reduce(self, bigram, values):\n",
      "        yield bigram, sum(values)\n",
      "\n",
      "    def map_best(self, bigram, count):\n",
      "        yield None, (count, bigram)\n",
      "\n",
      "    def reduce_best(self, _, bigrams_count):\n",
      "        for count, bigram in sorted(bigrams_count, reverse=True)[:20]:\n",
      "            yield bigram, count\n",
      "\n",
      "    def steps(self):\n",
      "        return [\n",
      "            MRStep(mapper_init=self.map_init, mapper=self.map, reducer=self.reduce),\n",
      "            MRStep(mapper=self.map_best, reducer=self.reduce_best)\n",
      "        ]\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    StaticAnalysis.run()"
     ]
    }
   ],
   "source": [
    "!echo \"\\n========== Config: ===========\"\n",
    "!cat cfg3.conf\n",
    "!echo \"\\n========== Count:  ===========\"\n",
    "!cat staticanalysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ff661-90b9-44b6-9e6a-e2034fc707c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
